{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many agronomic terms appear in natural language in multiple forms, e.g.:\n",
    "* \"The awns are rough\", \"It has rough awns\", or \"It is rough-awned\". In all these cases, the plant part (PLAN), awn, is modified by an adjective, rough. The combination, \"rough\" + \"awn\" is a trait (TRAT).\n",
    "* \"early maturing\", \"matures early\". In these cases, a trait (TRAT), 'maturing' is modified by an adjective, early. This combination \"early\" + \"maturing\" is a compound trait (TRAT).\n",
    "\n",
    "In this notebook, we will run a small section of text against a trained NLP model, read the predictions, identify compoud traits based on the above rules, and output modified named entities in JSON format that include the compound traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Do a quick training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"agdata\"\n",
    "from src.trainNER import *\n",
    "\n",
    "# Depending on the nature of out training dataset, we might get warnings that the \n",
    "# data is not well formatted. Ignore those errors for now. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# If you are running the notebook for the first time, and you do not have \n",
    "# an already custom trained NER recognition model, you will need to uncomment \n",
    "# the lines below and first train the model\n",
    "\n",
    "#n_iter = 10\n",
    "#trainModel(None,output_dir,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP parse some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(output_dir)\n",
    "\n",
    "test_text = '''Kold is a six-rowed winter feed barley obtained from the cross Triumph/Victor. It was released by the Oregon AES in 1993. It has rough awns and the aleurone is white. It has low lodging, matures early and its yield is low. Crop Science 25:1123 (1985).'''\n",
    "\n",
    "doc = nlp(test_text)\n",
    "\n",
    "colors = {'ALAS':'BlueViolet','CROP': 'Aqua','CVAR':'Chartreuse','PATH':'red','PED':'orange','PLAN':'pink','PPTD':'brown','TRAT':'yellow'}\n",
    "cust_options = {'ents': ['ALAS','CROP','CVAR','PATH','PED','PLAN','PPTD','TRAT'], 'colors':colors}\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True, options=cust_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify compound traits ADJ + PLAN = TRAT\n",
    "## first flag entities and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "# Comment out the following due to lack of statistical training for NERModel\n",
    "# print(\"\\nNoun Chunks:\")\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#             chunk.root.head.text)\n",
    "    \n",
    "print(\"\\nParts of Speech:\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, identify clauses fitting ADJ + PLAN = TRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def compound_trait_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for PLAN entities with a preceding one-token adjective\n",
    "        # (e.g., 'rough awns')\n",
    "        if ent.label_ == \"PLAN\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            print('DEBUG: ', ent.text, ent.start, ent.label_, prev_token.text, prev_token.pos_, prev_token.dep_)\n",
    "            if prev_token.pos_ == 'ADJ' and prev_token.dep_ == 'amod':\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label='TRAT')\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "new_doc = compound_trait_entities(doc)\n",
    "print(new_doc)\n",
    "print(\"Entities:\")\n",
    "for ent in new_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the above as a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO in the future: Add the component after the named entity recognizer\n",
    "# nlp.add_pipe(compound_trait_entities, after='ner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is rigid. It only check for PLAN entities with a preceding one-token adjective. This needs to be generalized to explicitly include any dependency relationehips (e.g., 'the awns are rough' in addition to 'rough awns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_entities = [ent for ent in doc.ents if ent.label_ == \"PLAN\"]\n",
    "for ent in plan_entities:\n",
    "    # Because the entity is a spans, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    if head.lemma_ == \"be\":\n",
    "        # Check if the children contain an adjectival complement (acomp)\n",
    "        acomps = [token for token in head.children if token.dep_ == \"acomp\"]\n",
    "        for acomp in acomps:\n",
    "            print(acomp, ent, \"is a TRAT (trait)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick gut-check on spacy basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now all we need to do is add this piece of logic to the earlier compound_trait_entities function we will add to the pipeline. This handles phrases where the adjective doesn't precede the noun it modifies, and the original code already handles the phrases where the adjective directly precedes the noun it modifies. But first, let's make sure we understand [Tokens](https://spacy.io/api/token), [Spans](https://spacy.io/api/span) and [Docs](https://spacy.io/api/doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Figure out alternatives to creating a Span\n",
    "test_doc = nlp('This is a simple sentence used for testing.')\n",
    "# span = test_doc[1:4]\n",
    "# The next one doesn't assign the label\n",
    "# span = test_doc[1:4].char_span(0, 11, label=\"FRAG\")\n",
    "span = Span(test_doc, 1, 4, label=\"FRAG\")\n",
    "print(\"SPAN FRAG:\", span.text, span.label_)\n",
    "\n",
    "# Now figure out how to figure out the right token number in the doc for a token\n",
    "token = test_doc[3]\n",
    "print(\"Token:\", token.text, \"at position\", token.i)\n",
    "\n",
    "# What is the first token in the span?\n",
    "token = test_doc[span.start]\n",
    "print(\"Span start:\", token.text, \"at position\", token.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined approach: Handling ADJ PLAN = TRAT and PLAN (be) ADJ = TRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def adj_plan_entities(doc):\n",
    "    # only deals with ADJ PLAN = TRAT entities (e.g, 'rough awns')\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PLAN\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            # print('DEBUG: ', ent.text, ent.start, ent.label_, prev_token.text, prev_token.pos_, prev_token.dep_)\n",
    "            if prev_token.pos_ == 'ADJ' and prev_token.dep_ == 'amod':\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label='TRAT')\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "def plan_adj_entities(doc):\n",
    "    #only deals with PLAN (be) ADJ = TRAT entities (e.g., 'awns are rough')\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PLAN\":\n",
    "            # Because the entity is a spans, we need to use its root token. The head\n",
    "            # is the syntactic governor of the person, e.g. the verb\n",
    "            head = ent.root.head\n",
    "            # print('DEBUG: entity head lemma', head.lemma_)\n",
    "            if head.lemma_ == \"be\":\n",
    "                # Check if the children contain an adjectival complement (acomp)\n",
    "                acomps = [token for token in head.children if token.dep_ == \"acomp\"]\n",
    "                # CAVEAT 1: For now let's assume adjectives are single-word\n",
    "                # Later we should figure out the lowest and highest index among the adjectives\n",
    "                acomp = acomps[0]\n",
    "                # print('DEBUG: ', acomp, ent, \"is a new TRAT (trait)\")\n",
    "                # CAVEAT 2: The document remains unchanged, so the term that will get stored\n",
    "                # will be the original phrase 'awns are rough' instead of the standardized\n",
    "                # 'rough awns'\n",
    "                #\n",
    "                # Having trouble defining a span from the first token of the Span 'ent' to the last token of 'acomp'\n",
    "                # print('DEBUG: acomp', doc[acomp.i+1])\n",
    "                new_ent = Span(doc, doc[ent.start].i, acomp.i+1, label=\"TRAT\")\n",
    "                new_ents.append(new_ent)\n",
    "                # I would much rather overwrite the phrase as 'rough awns' like this DEBUG statement shows\n",
    "                # by printing acomp + ent\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "def compound_trait_entities(doc):\n",
    "    doc = adj_plan_entities(doc)\n",
    "    doc = plan_adj_entities(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "new_doc = compound_trait_entities(doc)\n",
    "print(new_doc)\n",
    "print(\"Entities:\")\n",
    "for ent in new_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify compound traits ADJ + TRAT = TRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def adj_ent_entities(doc):\n",
    "    # only deals with ADJ TRAT (or PLAN) = TRAT entities (e.g, 'low lodging' or 'rough awns')\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ('PLAN', 'TRAT') and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            # print('DEBUG: ', ent.text, ent.start, ent.label_, prev_token.text, prev_token.pos_, prev_token.dep_)\n",
    "            if prev_token.pos_ == 'ADJ' and prev_token.dep_ == 'amod':\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label='TRAT')\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "def trat_adj_entities(doc):\n",
    "    # only deals with TRAT ADJ = TRAT entities (e.g., 'matures early')\n",
    "    # CAVEAT: we still need to do deal with TRAT (be) ADJ (e.g., 'its protein levels are low')\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ('TRAT'):\n",
    "            next_token = doc[ent.start + 1]\n",
    "            # print('DEBUG: ', ent.text, ent.start, ent.label_, next_token.text, next_token.pos_, next_token.dep_)\n",
    "            if next_token.pos_ == 'ADV' and next_token.dep_ == 'advmod':\n",
    "                new_ent = Span(doc, ent.start, ent.end + 1, label='TRAT')\n",
    "                new_ents.append(new_ent)    \n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "def compound_trait_entities(doc):\n",
    "    doc = adj_ent_entities(doc)\n",
    "    doc = plan_adj_entities(doc)\n",
    "    doc = trat_adj_entities(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "new_doc = compound_trait_entities(doc)\n",
    "print(new_doc)\n",
    "print(\"Entities:\")\n",
    "for ent in new_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add our new compound trait function to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(compound_trait_entities, after='ner')\n",
    "doc = nlp(test_text)\n",
    "print(doc)\n",
    "print(\"Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparse pedigrees and journal entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's re-aquaint ourselves with the results format of pre-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preparse import *\n",
    "\n",
    "regex_ents = preparse(test_text)\n",
    "print(regex_ents, '\\n')\n",
    "print (regex_ents[test_text]['entity 1'], '\\n')\n",
    "print (regex_ents[test_text]['entity 1']['label'], '\\n')\n",
    "print (regex_ents[test_text]['entity 1']['substring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to figure out matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "tdoc = nlp(\"I do not care if you do not care\")\n",
    "substring = \"do not care\"\n",
    "sdoc = nlp(substring)\n",
    "pattern = [{\"ORTH\": token.text} for token in sdoc]\n",
    "#pattern = [{\"ORTH\": \"do\"}, {\"ORTH\": \"not\"}, {\"ORTH\": \"care\"}]\n",
    "matcher.add(\"do not care\", None, pattern)\n",
    "matches = matcher(tdoc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check for entities overlapping each new regex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def get_matched_spans(doc, substring):\n",
    "\n",
    "    sdoc = nlp(substring)\n",
    "    pattern = [{\"ORTH\": token.text} for token in sdoc] #allows matching multi-word pattern\n",
    "    matcher.add(substring, None, pattern)\n",
    "    \n",
    "    result = []\n",
    "    matches = matcher(doc)\n",
    "    print(matches)\n",
    "    for match_id, start, end in matches:\n",
    "#        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "#        print(match_id, string_id, start, end, span.text)\n",
    "        print(match_id, start, end, span.text)\n",
    "        result.append(span)\n",
    "    return result\n",
    "    \n",
    "tdoc = nlp(\"I don't care if you don't care\")\n",
    "span_res = get_matched_spans(tdoc, \"don't care\")\n",
    "[(i.text,i.start,i.end) for i in span_res]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_data = regex_ents[test_text]['entity 1']\n",
    "print(doc.text)\n",
    "print(ent_data['substring'])\n",
    "# the following returns a list of spans, each indexed as span_list[0].text, \n",
    "#   span_list[0].start, and span_list[0].end\n",
    "span_list = get_matched_spans(doc, ent_data['substring'])\n",
    "\n",
    "# Although there might be multiple matches, just work with the first one for now\n",
    "new_ent = Span(doc, span_list[0].start, span_list[0].end, label=ent_data['label'])\n",
    "\n",
    "print(\"SPAN FRAG:\", new_ent.text, new_ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's how we put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agParse import *\n",
    "nlp = spacy.load(output_dir)\n",
    "text = 'Kold is a six-rowed winter feed barley obtained from the cross Triumph/Victor. It was released by the Oregon AES in 1993. It has rough awns and the aleurone is white. It has low lodging, matures early and its yield is low. Crop Science 25:1123 (1985).'\n",
    "nlp.add_pipe(compound_trait_entities, after='ner')\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the basics working. But here is a recap on the caveats: We don't handle multiple-word adjectival or adverbial modifiers like 'mid to late maturity' and 'height is very low'. We also don't handle TRAT (be) ADJ constructs (e.g., 'its yield is low'). These will need to be added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
