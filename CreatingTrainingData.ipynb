{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to speed up the process of manually annotating named entities and having their positions in the paragraph identified and placed in a syntax usable by spaCy's NER training routine. This notebook describes how to manually annotate data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open PDF file, extract page two and display as sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  24  early to midseason maturity.\n",
      "1 :  Spikes are medium lax, mid-long, inclined to nodding.\n",
      "2 :  It has semi-smooth awns.\n",
      "3 :  Glumes have a band of long hairs and the glume awn is three times the length of the glume.\n",
      "4 :  Hulls are adhering and slightly to semi-wrinkled.\n",
      "5 :  It has long rachilla hairs and colorless aleurone.\n",
      "6 :  Lateral veins are moderately prominent and the central vein is broad and raised over the germ but often indistinct at the center of the kernel.\n",
      "7 :  There are several barbs on lateral veins.\n",
      "8 :  The crease is V-shaped from the base and relatively narrow.\n",
      "9 :  Kernels are plump, wide at the center, tapering uniformly to both ends.\n",
      "10 :  Lateral kernels are moderately twisted.\n",
      "11 :  At the time of release it was resistant to stem rust, tolerant to BYD, moderately susceptible to spot blotch, net blotch and loose smut, and susceptible to Septoria leaf blotch and powdery mildew.\n",
      "12 :  It was evaluated as Entry 234 in the UC Regional Cereal Testing program for spring planting in the intermountain region of northern California.\n",
      "13 :  Crop Science 4:239 (1964)    \n",
      "14 :  LEGACY  \n",
      "15 :  Legacy is a six-rowed spring malting barley.\n",
      "16 :  It was released by Busch Agricultural Resources in 2000.\n",
      "17 :  It was selected from the cross 6B86-3517\n",
      "18 :  /Excel, a complex cross involving the parental cultivars Bumper, Karl, Manker, and Excel.\n",
      "19 :  Its experimental designation was 6B93-2978.\n",
      "20 :  Plants are tall with fair to poor lodging resistance and early maturity.\n",
      "21 :  It has semi-smooth awns, long rachilla hairs, and colorless aleurone.\n",
      "22 :  It is resistant to spot blotch and has slightly better net blotch resistance than Robust.\n",
      "23 :  At the time of evaluation it was susceptible to scald, BYD, and stripe rust.\n",
      "24 :  It was evaluated as Entry 1084 in the UC Regional Cereal Testing program from 2001-2011 for spring planting in the intermountain region of northern California.     \n",
      "25 :  LEWIS  \n",
      "26 :  Lewis is a two-rowed spring feed and malting barley.\n",
      "27 :  It was released by the USDA-ARS and the Montana AES in 1985.\n",
      "28 :  It was selected from the cross Hector/Klages.\n",
      "29 :  Its experimental designation was MT 547123.\n",
      "30 :  It is recommended as a feed barley under irrigation in Montana.\n",
      "31 :  It has higher yields than Klages in both dryland and irrigated trials and similar malting quality.\n",
      "32 :  It has midseason maturity and heads 3 days earlier than Klages.\n",
      "33 :  It has 11% more plump kernels, similar height and better\n",
      "34 :  lodging resistance than Klages.\n",
      "35 :  Its spikes are mid-lax, mid-long and nodding with rough awns.\n",
      "36 :  Glume awns are equal in length to the glume, and glumes are covered with long hairs.\n",
      "37 :  Rachis edges have long hairs.\n",
      "38 :  It has finely wrinkled hulls without barbs on the lateral veins, a long-haired rachilla and white aleurone.\n",
      "39 :  The crease is narrow at the base, tending to flare at the awn end.\n",
      "40 :  It has more tolerance to spot and net blotch and common root rot than Klages.\n",
      "41 :  It was evaluated as Entry 686 in the UC Regional Cereal Testing program from 1984-1987 for spring planting in the intermountain region of northern California.\n",
      "42 :  Crop Science 25:570-571 (1985)    \n",
      "43 :  LINDY  \n",
      "44 :  Lindy is a six-rowed spring feed barley.\n",
      "45 :  It was developed by CENEX, Oregon.\n",
      "46 :  It is late maturing and has tall plant height and poor straw strength.\n",
      "47 :  At the time of evaluation it was resistant to scald and leaf rust and susceptible to BYD.\n",
      "48 :  It was evaluated as Entry 700 in the UC Regional Cereal Testing program from 1985-1987 for spring planting in the intermountain area of northern California and for late fall planting in the Central Valley and the south-central coastal regions of California.    \n",
      "49 :  LUD  Lud is a two-rowed spring feed barley.\n",
      "50 :  It was released by North American Plant Breeders in 1975.\n",
      "51 :  It was selected from the cross Vada/Zephyr//RMGH 59114.\n",
      "52 :  It has midseason maturity (similar to Klages), medium short plant height (2-4 inches shorter than Klages) and strong straw.\n",
      "53 :  It has long, rough awns.\n",
      "54 :  Glumes have short hairs confined to the midline.\n",
      "55 :  Hulls are adhering and slightly wrinkled.\n",
      "56 :  The aleurone is colorless.\n",
      "57 :  Rachilla hairs are short.\n",
      "58 :  Veins are prominent and there are no barbs on lateral veins.\n",
      "59 :  The crease is closed in the lower one-third of the kernel, flaring at the awn end.\n",
      "60 :  At the time of release it was resistant to powdery mildew, loose smut, and covered smut and susceptible to scald, stem rust, net blotch and spot blotch.\n",
      "61 :  It was evaluated as Entry 582 in the UC Regional Cereal Testing program from 1980-1985 for spring planting in the intermountain region of northern California.     \n",
      "62 :  MADERA  Madera a six-rowed spring feed barley.\n",
      "63 :  It was released by Western Plant Breeders in 1992.\n",
      "64 :  It was selected from a male-sterile facilitated recurrent selection population (UZU-POP).\n",
      "65 :  Its experimental designation was DA 587- 71.\n",
      "66 :  It is late maturing and has medium plant height and fair to poor straw strength.\n",
      "67 :  At the time of evaluation it was moderately resistant to scald, net blotch, BYD, stripe rust and powdery mildew, and moderately susceptible to leaf rust.\n",
      "68 :  It was evaluated as Entry 814 in the UC Regional Cereal Testing program from 1990-1995 for late fall planting in the Central Valley and the south-central coastal regions of California.   \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "\n",
    "# Load language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "filename = \"Data/DavisLJ11/BarCvDescLJ11.pdf\"\n",
    "\n",
    "#Open PDF file for reading\n",
    "pdfFile = open(filename, mode=\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "\n",
    "# Select a page to work on\n",
    "pageNumber = 24\n",
    "\n",
    "# Get text\n",
    "OnePage = pdfReader.getPage(pageNumber-1) #0-based count\n",
    "OnePageText = OnePage.extractText()\n",
    "\n",
    "# Close PDF file\n",
    "pdfFile.close()\n",
    "\n",
    "OnePageText = OnePageText.replace('\\n','')\n",
    "\n",
    "# create a spaCy doc object from the page and break it into sentences\n",
    "doc = nlp(OnePageText)\n",
    "l=0\n",
    "for sent in doc.sents:\n",
    "     print(l, \": \", sent)\n",
    "     l = l+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Per-line named entity file and match entities to sentence positions.\n",
    "# TODO: The current approach relies on us manually looking at the output above and creating the _ner.txt by hand. This just won't work. Instead of this approach, we will start by automatic this step by using whatever model we currently have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/DavisLJ11/temp/barley_p24_ner.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-08cc3b74e95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#  1      Agri-Food Candada   ORG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#  1      1997    DATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/DavisLJ11/temp/barley_p24_ner.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "\n",
    "# NOTE: We are creating a file in the folder Data/DavisLJ11/temp. Make sure this folder\n",
    "# exists before running this code. The reason we have the temp subfolder is because this is \n",
    "# for testing. The file we be overwritten everytime we run this notebook. \n",
    "fname = \"Data/DavisLJ11/temp/barley_p\"+str(pageNumber)+\"_ner.txt\" \n",
    "\n",
    "# Covert the nlp sentence generator into a list of sentences\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Open the file of manually matched pairs (sentence # <tab> word phrase <tab> named entity)\n",
    "# e.g.:\n",
    "#  0      AC Metcalfe     CVAR\n",
    "#  0      two-rowed       TRAT\n",
    "#  0      barley          CROP\n",
    "#  1      Agri-Food Candada   ORG\n",
    "#  1      1997    DATE\n",
    "file = open(fname)\n",
    "reader = csv.reader(file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "data = list()\n",
    "\n",
    "for row in reader:\n",
    "    try:\n",
    "        (sentIndex, phrase, label) = row\n",
    "        sent = sentences[int(sentIndex)].string.rstrip()\n",
    "        \n",
    "        # find all instances of the 'phrase' in the 'sent'.\n",
    "        iter = re.finditer(r\"\\b\"+phrase+r\"\\b\", sent)\n",
    "        indices = [m.start(0) for m in iter]\n",
    "        \n",
    "        # check to make sure the phrase the user said was there was indeed found\n",
    "        if len(indices) == 0:\n",
    "            raise ValueError\n",
    "                \n",
    "        # print out all instances\n",
    "        for i in indices:\n",
    "#            print(sentIndex, sent, phrase, \"(\"+str(i), i+len(phrase), \"'\"+label+\"')\")\n",
    "            data.append([sentIndex, sent, phrase, \"(\"+str(i)+\", \"+str(i+len(phrase))+\", '\"+label+\"')\"])\n",
    "        \n",
    "            \n",
    "    except:\n",
    "        print(\"Handle manually: \", row)\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame(data, columns = [\"Index\", \"Sentence\", \"Phrase\", \"MatchInfo\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to clean up overlapping intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "coordRegex = re.compile(r'(\\d+), (\\d+)')\n",
    "\n",
    "def sortByStart(coords):\n",
    "    \"\"\"For use in sort routines, return object with lowest (X,Y) values\"\"\"\n",
    "    # split out coordinates that come in as (5, 7, 'CVAR')\n",
    "    mo = coordRegex.search(coords)\n",
    "    return(int(mo.group(1)))\n",
    "\n",
    "def overlaps(coord1, coord2):\n",
    "    \"\"\"Check if coordinates of the form 5, 7, 'CVAR' and 32, 46, 'TRAT' overlap\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    "    \n",
    "    if ((coord1High >= coord2Low) and (coord1Low <= coord2Low) or\n",
    "        (coord2High >= coord1Low) and (coord2Low <= coord1Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def keepFirst(coord1, coord2):\n",
    "    \"\"\"Given overlapping coordinates, return the wider encompassing one.\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    " \n",
    "    if (int(coord1High) - int(coord1Low)) >= (int(coord2High) - int(coord2Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# print(\"Should be false:\", overlaps(\"(5, 7, 'CVAR')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(26, 46, 'TRAT')\"))\n",
    "# print(\"Keeper:\", keepFirst(\"34, 46, 'TRAT'\", \"34, 46, 'TRAT'\"))\n",
    "\n",
    "def cleanIntervals(inputString=\"\"):\n",
    "    \"\"\"order intervals like (5, 7, 'CVAR'), (32, 46, 'TRAT'), (26, 46, 'TRAT') and remove overlapping ones.\"\"\"\n",
    "    inputString = inputString.lstrip(\"(\").rstrip(\")\")\n",
    "    intervalList = inputString.split(\"), (\")\n",
    "    intervalList.sort(key = sortByStart)\n",
    "#    print(\"Sorted Interval List:\", intervalList)\n",
    "\n",
    "    # Pairwise compare every interval in the list to every other interval to check overlap\n",
    "    keeperList = [True]*len(intervalList) # Logic array to determine if each interval should be kept\n",
    "    i=0\n",
    "    for interval1 in intervalList:\n",
    "        for interval2 in intervalList:\n",
    "            if interval1 == interval2:\n",
    "                if intervalList.index(interval1) != i: # when both are the same we reject the higher one\n",
    "                    keeperList[i] = False\n",
    "            else:\n",
    "                if overlaps(interval1, interval2) and keepFirst(interval1, interval2) == False:\n",
    "                    keeperList[i] = False\n",
    "        i = i+1\n",
    "        \n",
    "#    print(\"keeperList:\", keeperList)\n",
    "   \n",
    "    # Build up the return interval list\n",
    "    returnStr = \"(\"\n",
    "    for interval, isKeeper in zip(intervalList, keeperList):\n",
    "        if isKeeper:\n",
    "            returnStr = returnStr + interval + \"), (\"\n",
    "    return (returnStr.rstrip(\"), (\") + \")\")\n",
    "        \n",
    "# cleanIntervals(\"(5, 7, 'CVAR'), (32, 46, 'TRAT'), (5, 9, 'CVAR'), (48, 55, 'ORG'), (26, 46, 'TRAT')\")\n",
    "# cleanIntervals(\"(0, 8, 'CVAR'), (0, 5, 'CVAR'), (21, 26, 'PLAN'), (32, 37, 'CVAR')\")\n",
    "#cleanIntervals(\"(0, 12, 'CVAR'), (39, 49, 'CVAR'), (39, 49, 'CVAR'), (71, 77, 'CVAR'), (71, 77, 'CVAR'), (92, 113, 'TRAT'), (140, 150, 'CVAR'), (140, 150, 'CVAR'), (181, 187, 'CVAR'), (181, 187, 'CVAR')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all matches for each sentence on a single line and output in spaCy training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use Pandas dataframes to aggregate all entity matches together for a single sentence\n",
    "agg_rules = {'Sentence': 'first', 'Phrase': 'first', 'MatchInfo': lambda x: ', '.join(x)}\n",
    "res = df.groupby('Index').agg(agg_rules)\n",
    "#print(res)\n",
    "\n",
    "# Now format it just like what is needed for the spaCy training module: \n",
    "# E.g.:\n",
    "# ('Eight-Twelve is a six-rowed winter feed barley', {'entities': [(0, 12, 'CVAR'), (18, 27, 'TRAT'), (28, 39, 'TRAT'),(40, 46, 'CROP')]}),\n",
    "records = res.to_dict('records')\n",
    "\n",
    "out_fname = \"Data/UIdaho2019/small_grains_report_2019_p\"+str(pageNumber)+\"_td.py\"\n",
    "fo = open(out_fname, 'w')\n",
    "fo.write(\"TRAIN_DATA = [\")\n",
    "maxr = len(records)\n",
    "for i in range(0,maxr):\n",
    "    fo.write(\"    ('\"+records[i]['Sentence']+\"', {'entities': [\"+cleanIntervals(records[i]['MatchInfo'])+\"]})\")\n",
    "    if (i == maxr-1):\n",
    "        fo.write(\"\\n\")\n",
    "    else:\n",
    "        fo.write(\",\\n\")\n",
    "\n",
    "fo.write(\"]\\n\")\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above content will be written to a file e.g., `Data/UIdaho2019/small_grains_report_2019_p50_td.py`. You can add any manual corrections (usually PED and JRNL entries) and then running the script `python3 py2json.py --doc 'Data/UIdaho2019/small-grains-report_2019.pdf' --url 'https://www.uidaho.edu/-/media/UIdaho-Responsive/Files/Extension/topic/cereals/scse/2019/small-grains-report_2019.pdf' --chunk 50 Data/UIdaho2019/small_grains_report_2019_p50_td.py Data/UIdaho2019/small_grains_report_2019_p50_td.json` to create the JSON file for Training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
