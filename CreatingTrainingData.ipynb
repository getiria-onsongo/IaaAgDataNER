{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to speed up the process of manually annotating named entities and having their positions in the paragraph identified and placed in a syntax usable by spaCy's NER training routine. This notebook describes how to manually annotate data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open PDF file, extract page two and display as sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "\n",
    "# Load language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pdfFilename = \"Data/DavisLJ11/BarCvDescLJ11.pdf\"\n",
    "\n",
    "#Open PDF file for reading\n",
    "pdfFile = open(pdfFilename, mode=\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "\n",
    "# Select a page to work on\n",
    "pageNumber = 20\n",
    "\n",
    "# Get text\n",
    "OnePage = pdfReader.getPage(pageNumber-1) #0-based count\n",
    "OnePageText = OnePage.extractText()\n",
    "\n",
    "# Close PDF file\n",
    "pdfFile.close()\n",
    "\n",
    "OnePageText = OnePageText.replace('\\n','')\n",
    "\n",
    "# create a spaCy doc object from the page and break it into sentences\n",
    "doc = nlp(OnePageText)\n",
    "l=0\n",
    "for sent in doc.sents:\n",
    "     print(l, \": \", sent)\n",
    "     l = l+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Per-line named entity file and match entities to sentence positions.\n",
    "# TODO: The current approach relies on us manually looking at the output above and creating the \\*_ner.txt file by hand. This just won't work scale. Instead of this approach, the very first thing we should do is automate this step by using whatever model we currently have to pre-annotate. What would even be better is having an interface that will make it easy to correct pre-annotations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "\n",
    "# NOTE: We are creating a file in the folder Data/DavisLJ11/test. Make sure this folder\n",
    "# exists before running this code. The reason we have the temp subfolder is because this is \n",
    "# for testing. The file we be overwritten everytime we run this notebook. \n",
    "fname = \"Data/DavisLJ11/test/barley_p\"+str(pageNumber)+\"_ner.txt\" \n",
    "\n",
    "# Output where we will write training data\n",
    "out_fname = \"Data/DavisLJ11/test/barley_p\"+str(pageNumber)+\"_td.py\"\n",
    "\n",
    "# Covert the nlp sentence generator into a list of sentences\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Open the file of manually matched pairs (sentence # <tab> word phrase <tab> named entity)\n",
    "# e.g.:\n",
    "#  0      AC Metcalfe     CVAR\n",
    "#  0      two-rowed       TRAT\n",
    "#  0      barley          CROP\n",
    "#  1      Agri-Food Candada   ORG\n",
    "#  1      1997    DATE\n",
    "file = open(fname)\n",
    "reader = csv.reader(file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "data = list()\n",
    "\n",
    "for row in reader:\n",
    "    try:\n",
    "        (sentIndex, phrase, label) = row\n",
    "        sent = sentences[int(sentIndex)].string.strip()\n",
    "        \n",
    "        \n",
    "        # find all instances of the 'phrase' in the 'sent'.\n",
    "        iter = re.finditer(r\"\\b\"+phrase+r\"\\b\", sent)\n",
    "        indices = [m.start(0) for m in iter]\n",
    "        \n",
    "        # check to make sure the phrase the user said was there was indeed found\n",
    "        if len(indices) == 0:\n",
    "                raise ValueError\n",
    "                \n",
    "        # print out all instances\n",
    "        for i in indices:\n",
    "#            print(sentIndex, sent, phrase, \"(\"+str(i), i+len(phrase), \"'\"+label+\"')\")\n",
    "            data.append([sentIndex, sent, phrase, \"(\"+str(i)+\", \"+str(i+len(phrase))+\", '\"+label+\"')\"])\n",
    "        \n",
    "            \n",
    "    except:\n",
    "        print(\"Handle manually: \", row)\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame(data, columns = [\"Index\", \"Sentence\", \"Phrase\", \"MatchInfo\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to clean up overlapping intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "coordRegex = re.compile(r'(\\d+), (\\d+)')\n",
    "\n",
    "def sortByStart(coords):\n",
    "    \"\"\"For use in sort routines, return object with lowest (X,Y) values\"\"\"\n",
    "    # split out coordinates that come in as (5, 7, 'CVAR')\n",
    "    mo = coordRegex.search(coords)\n",
    "    return(int(mo.group(1)))\n",
    "\n",
    "def overlaps(coord1, coord2):\n",
    "    \"\"\"Check if coordinates of the form 5, 7, 'CVAR' and 32, 46, 'TRAT' overlap\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    "    \n",
    "    if ((coord1High >= coord2Low) and (coord1Low <= coord2Low) or\n",
    "        (coord2High >= coord1Low) and (coord2Low <= coord1Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def keepFirst(coord1, coord2):\n",
    "    \"\"\"Given overlapping coordinates, return the wider encompassing one.\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    " \n",
    "    if (int(coord1High) - int(coord1Low)) >= (int(coord2High) - int(coord2Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# print(\"Should be false:\", overlaps(\"(5, 7, 'CVAR')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(26, 46, 'TRAT')\"))\n",
    "# print(\"Keeper:\", keepFirst(\"34, 46, 'TRAT'\", \"34, 46, 'TRAT'\"))\n",
    "\n",
    "def cleanIntervals(inputString=\"\"):\n",
    "    \"\"\"order intervals like (5, 7, 'CVAR'), (32, 46, 'TRAT'), (26, 46, 'TRAT') and remove overlapping ones.\"\"\"\n",
    "    inputString = inputString.lstrip(\"(\").rstrip(\")\")\n",
    "    intervalList = inputString.split(\"), (\")\n",
    "    intervalList.sort(key = sortByStart)\n",
    "#    print(\"Sorted Interval List:\", intervalList)\n",
    "\n",
    "    # Pairwise compare every interval in the list to every other interval to check overlap\n",
    "    keeperList = [True]*len(intervalList) # Logic array to determine if each interval should be kept\n",
    "    i=0\n",
    "    for interval1 in intervalList:\n",
    "        for interval2 in intervalList:\n",
    "            if interval1 == interval2:\n",
    "                if intervalList.index(interval1) != i: # when both are the same we reject the higher one\n",
    "                    keeperList[i] = False\n",
    "            else:\n",
    "                if overlaps(interval1, interval2) and keepFirst(interval1, interval2) == False:\n",
    "                    keeperList[i] = False\n",
    "        i = i+1\n",
    "        \n",
    "#    print(\"keeperList:\", keeperList)\n",
    "   \n",
    "    # Build up the return interval list\n",
    "    returnStr = \"(\"\n",
    "    for interval, isKeeper in zip(intervalList, keeperList):\n",
    "        if isKeeper:\n",
    "            returnStr = returnStr + interval + \"), (\"\n",
    "    return (returnStr.rstrip(\"), (\") + \")\")\n",
    "        \n",
    "# cleanIntervals(\"(5, 7, 'CVAR'), (32, 46, 'TRAT'), (5, 9, 'CVAR'), (48, 55, 'ORG'), (26, 46, 'TRAT')\")\n",
    "# cleanIntervals(\"(0, 8, 'CVAR'), (0, 5, 'CVAR'), (21, 26, 'PLAN'), (32, 37, 'CVAR')\")\n",
    "#cleanIntervals(\"(0, 12, 'CVAR'), (39, 49, 'CVAR'), (39, 49, 'CVAR'), (71, 77, 'CVAR'), (71, 77, 'CVAR'), (92, 113, 'TRAT'), (140, 150, 'CVAR'), (140, 150, 'CVAR'), (181, 187, 'CVAR'), (181, 187, 'CVAR')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all matches for each sentence on a single line and output in spaCy training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use Pandas dataframes to aggregate all entity matches together for a single sentence\n",
    "agg_rules = {'Sentence': 'first', 'Phrase': 'first', 'MatchInfo': lambda x: ', '.join(x)}\n",
    "res = df.groupby('Index').agg(agg_rules)\n",
    "#print(res)\n",
    "\n",
    "# Now format it just like what is needed for the spaCy training module: \n",
    "# E.g.:\n",
    "# ('Eight-Twelve is a six-rowed winter feed barley', {'entities': [(0, 12, 'CVAR'), (18, 27, 'TRAT'), (28, 39, 'TRAT'),(40, 46, 'CROP')]}),\n",
    "records = res.to_dict('records')\n",
    "\n",
    "fo = open(out_fname, 'w')\n",
    "fo.write(\"TRAIN_DATA = [\")\n",
    "maxr = len(records)\n",
    "for i in range(0,maxr):\n",
    "    fo.write(\"    ('\"+records[i]['Sentence']+\"', {'entities': [\"+cleanIntervals(records[i]['MatchInfo'])+\"]})\")\n",
    "    if (i == maxr-1):\n",
    "        fo.write(\"\\n\")\n",
    "    else:\n",
    "        fo.write(\",\\n\")\n",
    "\n",
    "fo.write(\"]\\n\")\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above content will be written to a file (see out_fname) e.g., `Data/DavisLJ11/test/barley_p20_td.py`. You can add any manual corrections (usually PED and JRNL entries) and then running the script `python3 src/py2json.py --doc 'Data/DavisLJ11/BarCvDescLJ11.pdf' --url 'https://smallgrains.ucdavis.edu/cereal_files/BarCvDescLJ11.pdf' --chunk 20 Data/DavisLJ11/test/barley_p20_td.py Data/DavisLJ11/test/barley_p20_td.json` to create the JSON file for Training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
