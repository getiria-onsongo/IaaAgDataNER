{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a href='https://agroinformatics.org'> <img src='GEMS-logo2d.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Basics\n",
    "\n",
    "**spaCy** (https://spacy.io/) is an open-source Python library that parses and \"understands\" large volumes of text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Setup\n",
    "\n",
    "Installation is a two-step process. First, install spaCy using either conda or pip. Next, download the specific model you want, based on language.<br> For more info visit https://spacy.io/usage/\n",
    "\n",
    "### 1. From the command line or terminal:\n",
    "> `conda install -c conda-forge spacy`\n",
    "> <br>*or*<br>\n",
    "> `pip install -U spacy`\n",
    "\n",
    "> ### Alternatively you can create a virtual environment:\n",
    "> `conda create -n spacyenv python=3 spacy=2`\n",
    "\n",
    "### 2. Next, also from the command line (you must run this as admin or use sudo):\n",
    "\n",
    "> `python -m spacy download en`\n",
    "\n",
    "> ### If successful, you should see a message like:\n",
    "\n",
    "> **`Linking successful`**<br>\n",
    "> `    PathToSpacyEnvironment\\spacyenv\\lib\\site-packages\\en_core_web_sm -->`<br>\n",
    "> `    PathToSpacyEnvironment\\spacyenv\\lib\\site-packages\\spacy\\data\\en`<br>\n",
    "> ` `<br>\n",
    "> `    You can now load the model via spacy.load('en')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with spaCy in Python\n",
    "\n",
    "This is a typical set of instructions for importing and working with spaCy. Will likely take awhile - spaCy has a fairly large library to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'G.E.M.S is applying for an NSF grant worth $5 million. Good luck!')\n",
    "\n",
    "# Check to see what components are current live\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G.E.M.S PROPN nsubj\n",
      "is AUX aux\n",
      "applying VERB ROOT\n",
      "for ADP prep\n",
      "an DET det\n",
      "NSF PROPN compound\n",
      "grant NOUN pobj\n",
      "worth ADJ amod\n",
      "$ SYM quantmod\n",
      "5 NUM compound\n",
      "million NUM npadvmod\n",
      ". PUNCT punct\n",
      "Good ADJ amod\n",
      "luck NOUN ROOT\n",
      "! PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the explain function in spacy to find out meaning of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away we see some interesting things happen:\n",
    "1. G.E.M.S is recognized to be a Proper Noun, not just a word at the start of a sentence\n",
    "2. G.E.M.S is kept together as one entity ('token'). Not broken on periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# spaCy Objects\n",
    "\n",
    "After importing the spacy module in the cell above we loaded a **model** and named it `nlp`.<br>Next we created a **Doc** object by applying the model to our text, and named it `doc`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Pipeline\n",
    "When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.   Image source: https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see what components currently live in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x108a50828>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x108b92fa8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x108bac048>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Tokenization\n",
    "The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G.E.M.S PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "   SPACE \n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "hiring NOUN pcomp\n",
      "anymore ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# NOTE: I have no idea if this is true. Kevin and Jesse would know better. Just demonstrating Spacy. \n",
    "doc2 = nlp(u\"G.E.M.S isn't   looking into hiring anymore.\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `isn't` has been split into two tokens. spaCy recognizes both the root verb `is` and the negation attached to it. Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens. Spacy is able to recognize the extended space is different than space between words. For texts such as poems this might be useful.v\n",
    "\n",
    "NOTE: Even though `doc2` contains processed information about each token, it also retains the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G.E.M.S isn't   looking into hiring anymore."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G.E.M.S"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Part-of-Speech Tagging (POS)\n",
    "The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `G.E.M.S` was recognized to be a ***proper noun***. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns. A big advantage of using Spacy is we did not have to do any training here. \n",
    "\n",
    "For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dependencies\n",
    "In addition to tagging, Spacy assigns syntactic dependencies to each token. This is the relationship between words. Notice below Spacy is able to recognize the word/token Great has two different relationships in the two sentences. \n",
    "\n",
    "For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "<br>A good explanation of typed dependencies can be found [here](https://nlp.stanford.edu/software/dependencies_manual.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Great Engineers are in demand.\")\n",
    "doc3 = nlp(u\"Great Britain is in trouble.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great , PROPN , amod\n"
     ]
    }
   ],
   "source": [
    "print(doc2[0].text, \",\", doc2[0].pos_, \",\", doc2[0].dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great , PROPN , compound\n"
     ]
    }
   ],
   "source": [
    "print(doc3[0].text, \",\", doc3[0].pos_, \",\", doc3[0].dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full name of a tag use `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ = adjective\n",
      "amod = adjectival modifier\n",
      "PROPN = proper noun\n",
      "compound = compound\n"
     ]
    }
   ],
   "source": [
    "print(\"ADJ =\",spacy.explain('ADJ'))\n",
    "print(\"amod =\",spacy.explain('amod'))\n",
    "print(\"PROPN =\",spacy.explain('PROPN'))\n",
    "print(\"compound =\",spacy.explain('compound'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"01b8a0d782b04b60b7697cdc33b5a7e9-0\" class=\"displacy\" width=\"600\" height=\"192.0\" direction=\"ltr\" style=\"max-width: none; height: 192.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Great</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">Engineers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">demand.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-0\" stroke-width=\"2px\" d=\"M70,57.0 C70,2.0 160.0,2.0 160.0,57.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,59.0 L62,47.0 78,47.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-1\" stroke-width=\"2px\" d=\"M180,57.0 C180,2.0 270.0,2.0 270.0,57.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M180,59.0 L172,47.0 188,47.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-2\" stroke-width=\"2px\" d=\"M290,57.0 C290,2.0 380.0,2.0 380.0,57.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,59.0 L388.0,47.0 372.0,47.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-3\" stroke-width=\"2px\" d=\"M400,57.0 C400,2.0 490.0,2.0 490.0,57.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b8a0d782b04b60b7697cdc33b5a7e9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M490.0,59.0 L498.0,47.0 482.0,47.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import visualization package\n",
    "from spacy import displacy\n",
    "\n",
    "# Render the dependency parse immediately inside Jupyter:\n",
    "displacy.render(doc2, style='dep', jupyter=True, options={'distance': 110})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Additional Token Attributes\n",
    "Some of the other information that spaCy assigns to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(doc2[2].text)\n",
    "print(doc2[2].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUX\n",
      "VBP / verb, non-3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(doc2[2].pos_)\n",
    "print(doc2[2].tag_ + ' / ' + spacy.explain(doc2[2].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Boolean Values:\n",
    "print(doc2[0].is_alpha)\n",
    "print(doc2[0].is_stop)\n",
    "print(doc2[3].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSF - ORG - Companies, agencies, institutions, etc.\n",
      "$5 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'G.E.M.S is applying for an NSF grant worth $5 million.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">G.E.M.S is applying for an \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NSF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " grant worth \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $5 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Spans\n",
    "Large Doc objects can be hard to work with at times. A **span** is a slice of Doc object in the form `Doc[start:stop]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'One of the reasons I enjoy working with this group is its diverse pool of talent.  \\\n",
    "To quote Karl Popper “Disciplines are distinguished partly for historical reasons and reasons of \\\n",
    "administrative convenience.......We are not students of some subject matter but students of problems. \\\n",
    "And problems may cut right across the borders of any subject matter or discipline” ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karl Popper\n"
     ]
    }
   ],
   "source": [
    "quote_source = doc3[20:22]\n",
    "print(quote_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quote_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Generators\n",
    "For efficiency, Spacy uses a lot of generators. This reduces memory foot print. Data are not generated unti when they are needed. <br/>\n",
    "\n",
    "### Example: Given a list x = [1, 2, 3] <br/>\n",
    "We know a user might need to use the square of the first two items. \n",
    "\n",
    "### Solution 1: Write a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def doubleListFunc(lst):\n",
    "    newList = []\n",
    "    for item in lst:\n",
    "        newList.append(item * 2)\n",
    "    return newList\n",
    "\n",
    "x = [4, 12, 3]\n",
    "x1 = doubleListFunc(x) # This solution doubles everything in the list even if we might only need 1st item\n",
    "print(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Write a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doubleListGen(lst):\n",
    "    for item in lst:\n",
    "        yield item * 2\n",
    "        \n",
    "x = [4, 12, 3]\n",
    "our_generator = doubleListGen(x)\n",
    "next(our_generator) # Gets excuted when the item is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(our_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(our_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(our_generator) # This will give an error. Generator got to the end of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a generator, if you need all the items, you will need a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = [4, 12, 3]\n",
    "x2 = []\n",
    "our_generator = doubleListGen(x)\n",
    "for x in our_generator:\n",
    "    x2.append(x)\n",
    "print(x2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = [4, 12, 3]\n",
    "our_generator = doubleListGen(x)\n",
    "x2 = [x for x in our_generator]\n",
    "print(x2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Sentences\n",
    "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "#doc4.sents[0]\n",
    "for sent in doc4.sents:\n",
    "     print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Barley Related Ag Data\n",
    "\n",
    "Instructions for extracting agricultural related data from descriptions of barley cultivars. Before extracting ag data, we need to add new named entities to the training model. These entities are: <br/>\n",
    "ALAS = varietal_alias <br/>\n",
    "CROP = crop <br/>\n",
    "CVAR = crop_variety <br/>\n",
    "JRNL = journal_reference <br/>\n",
    "PATH = pathogen <br/>\n",
    "PED  = pedigree <br/>\n",
    "PLAN = plant_anatomy <br/>\n",
    "PPTD = plant_predisposition_to_disease <br/>\n",
    "TRAT = trait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (trainNER.py, line 11)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-22680d9fc053>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from trainNER import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/gonsongo/Desktop/research/iaa/Projects/python/IaaAgDataNER/trainNER.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    from __future__ import unicode_literals, print_function\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "from trainNER import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to recognized ag data entities e.g., TRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"agdata\"\n",
    "output_dir=\"/Users/gonsongo/Desktop/research/iaa/Projects/python/IaaAgDataNER\"\n",
    "n_iter = 100\n",
    "trainModel(model,output_dir,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and do a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agdata_nlp = spacy.load(output_dir)\n",
    "test_text = '''Kold is a six-rowed winter feed barley. It was released by the Oregon and Idaho AESs in 1993'''\n",
    "doc = agdata_nlp(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePEDtokens(doc):\n",
    "    ''' By default spacy splits PED into different tokens. This\n",
    "    function takes as input a spacy doc and merges PEDs into a\n",
    "    single token. '''\n",
    "    indexes = [m.span() for m in re.finditer('[\\w|\\S]+/[\\w|S]+', doc.text, flags=re.IGNORECASE)]\n",
    "    for start, end in indexes:\n",
    "        doc.merge(start_idx=start, end_idx=end)\n",
    "    return doc\n",
    "\n",
    "agdata_nlp.add_pipe(mergePEDtokens, before='ner')\n",
    "agdata_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the displaCy library\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on a PDF file\n",
    "### Open PDF file, extract one page, classify and display  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open PDF file for reading\n",
    "pdfFile = open(\"BarCvDescLJ11.pdf\", mode=\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "\n",
    "# Find the total number \n",
    "numPages = pdfReader.numPages\n",
    "\n",
    "# Randomly pick one\n",
    "pageNumber = random.randrange(0,numPages)\n",
    "\n",
    "# Get text\n",
    "OnePage = pdfReader.getPage(pageNumber)\n",
    "OnePageText = OnePage.extractText()\n",
    "\n",
    "# Close PDF file\n",
    "pdfFile.close()\n",
    "\n",
    "# Remove newlines. It appears multiple newlines together makes\n",
    "# Spacy think that is the end of a sentence. The PDF reader reads the text in\n",
    "# an odd fashion\n",
    "OnePageText = OnePageText.replace('\\n','')\n",
    "\n",
    "\n",
    "# Customize PDF\n",
    "colors = {'ALAS':'BlueViolet','CROP': 'Aqua','CVAR':'Chartreuse','PATH':'red','PED':'orange','PLAN':'pink','PPTD':'brown','TRAT':'yellow'}\n",
    "cust_options = {'ents': ['ALAS','CROP','CVAR','PATH','PED','PLAN','PPTD','TRAT'], 'colors':colors}\n",
    "\n",
    "# Doc.sents is a generator (get the sentences using list comprehension)¶\n",
    "# It is important to note that doc.sents is a generator. That is, a Doc\n",
    "# is not segmented until doc.sents is called. This means that, while you\n",
    "# could print the second Doc token with print(doc[1]), you can't call\n",
    "# the \"second Doc sentence\" with print(doc.sents[1]):\n",
    "\n",
    "agdata_nlp = spacy.load(output_dir)\n",
    "agdata_nlp.add_pipe(mergePEDtokens, before='ner')\n",
    "agdata_nlp.pipe_names\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(mergePEDtokens, before='tagger')\n",
    "doc = nlp(OnePageText)\n",
    "doc_sents = [sent for sent in doc.sents]\n",
    "for sent in doc_sents:\n",
    "    docx = agdata_nlp(sent.text)\n",
    "    if docx.ents:\n",
    "        displacy.render(docx, style='ent', jupyter=True, options=cust_options)\n",
    "    else:\n",
    "        print(docx.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need some customization: Steveland/Luther//Wintermalt is split into two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(mergePEDtokens, before='tagger')\n",
    "doc = nlp(\"'It was selected from the cross Steveland/Luther//Wintermalt'\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
