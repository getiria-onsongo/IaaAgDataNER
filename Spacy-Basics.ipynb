{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a href='https://agroinformatics.org'> <img src='images/GEMS-logo2d.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Basics\n",
    "\n",
    "**spaCy** (https://spacy.io/) is an open-source Python library that parses and \"understands\" large volumes of text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Setup\n",
    "\n",
    "Installation is a two-step process. First, install spaCy using either conda or pip. Next, download the specific model you want, based on language.<br> For more info visit https://spacy.io/usage/\n",
    "\n",
    "### 1. From the command line or terminal:\n",
    "> `conda install -c conda-forge spacy`\n",
    "> <br>*or*<br>\n",
    "> `pip install -U spacy`\n",
    "\n",
    "> ### Alternatively you can create a virtual environment:\n",
    "> `conda create -n spacyenv python=3 spacy=2`\n",
    "\n",
    "### 2. Next, also from the command line (you must run this as admin or use sudo):\n",
    "\n",
    "> `python -m spacy download en`\n",
    "\n",
    "> ### If successful, you should see a message like:\n",
    "\n",
    "> **`Linking successful`**<br>\n",
    "> `    PathToSpacyEnvironment\\spacyenv\\lib\\site-packages\\en_core_web_sm -->`<br>\n",
    "> `    PathToSpacyEnvironment\\spacyenv\\lib\\site-packages\\spacy\\data\\en`<br>\n",
    "> ` `<br>\n",
    "> `    You can now load the model via spacy.load('en')`\n",
    "\n",
    "### 3. To check version of spacy on the command line:\n",
    "`python3 -m spacy info`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with spaCy in Python\n",
    "\n",
    "This is a typical set of instructions for importing and working with spaCy. Will likely take awhile - spaCy has a fairly large library to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Import spaCy\n",
    "import spacy\n",
    "# Import visualization package\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "# Load the language library\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'G.E.M.S is applying for an NSF grant worth $5 million. Good luck!')\n",
    "\n",
    "# Check to see what components are current live\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the explain function in spacy to find out meaning of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away we see some interesting things happen:\n",
    "1. G.E.M.S is recognized to be a Proper Noun, not just a word at the start of a sentence\n",
    "2. G.E.M.S is kept together as one entity ('token'). Not broken on periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# spaCy Objects\n",
    "\n",
    "After importing the spacy module in the cell above we loaded a **model** and named it `nlp`.<br>Next we created a **Doc** object by applying the model to our text, and named it `doc`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Pipeline\n",
    "When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.   Image source: https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see what components currently live in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Tokenization\n",
    "The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I have no idea if this is true. Kevin and Jesse would know better. Just demonstrating Spacy. \n",
    "doc2 = nlp(u\"G.E.M.S isn't looking into hiring anymore.\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `isn't` has been split into two tokens. spaCy recognizes both the root verb `is` and the negation attached to it. Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens. Spacy is able to recognize the extended space is different than space between words. For texts such as poems this might be useful.v\n",
    "\n",
    "NOTE: Even though `doc2` contains processed information about each token, it also retains the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Part-of-Speech Tagging (POS)\n",
    "The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `G.E.M.S` was recognized to be a ***proper noun***. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns. A big advantage of using Spacy is we did not have to do any training here. \n",
    "\n",
    "For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dependencies\n",
    "In addition to tagging, Spacy assigns syntactic dependencies to each token. This is the relationship between words. Notice below Spacy is able to recognize the word/token Great has two different relationships in the two sentences. \n",
    "\n",
    "For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "<br>A good explanation of typed dependencies can be found [here](https://nlp.stanford.edu/software/dependencies_manual.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Great Engineers are in demand.\")\n",
    "doc3 = nlp(u\"Great Britain is in trouble.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc2[0].text, \",\", doc2[0].pos_, \",\", doc2[0].dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc3[0].text, \",\", doc3[0].pos_, \",\", doc3[0].dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full name of a tag use `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADJ =\",spacy.explain('ADJ'))\n",
    "print(\"amod =\",spacy.explain('amod'))\n",
    "print(\"PROPN =\",spacy.explain('PROPN'))\n",
    "print(\"compound =\",spacy.explain('compound'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the dependency parse immediately inside Jupyter:\n",
    "displacy.render(doc2, style='dep', jupyter=True, options={'distance': 150})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Additional Token Attributes\n",
    "Some of the other information that spaCy assigns to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(doc2[2].text)\n",
    "print(doc2[2].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(doc2[2].pos_)\n",
    "print(doc2[2].tag_ + ' / ' + spacy.explain(doc2[2].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Values:\n",
    "print(doc2[0].is_alpha)\n",
    "print(doc2[0].is_stop)\n",
    "print(doc2[3].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'G.E.M.S is applying for an NSF grant worth $5 million.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Spans\n",
    "Large Doc objects can be hard to work with at times. A **span** is a slice of Doc object in the form `Doc[start:stop]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'One of the reasons I enjoy working with this group is its diverse pool of talent.  \\\n",
    "To quote Karl Popper “Disciplines are distinguished partly for historical reasons and reasons of \\\n",
    "administrative convenience.......We are not students of some subject matter but students of problems. \\\n",
    "And problems may cut right across the borders of any subject matter or discipline” ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_source = doc3[20:22]\n",
    "print(quote_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(quote_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Generators\n",
    "For efficiency, Spacy uses a lot of generators. This reduces memory foot print. Data are not generated unti when they are needed. <br/>\n",
    "\n",
    "### Example: Given a list x = [1, 2, 3] <br/>\n",
    "We know a user might need to use the square of the first two items. \n",
    "\n",
    "### Solution 1: Write a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleListFunc(lst):\n",
    "    newList = []\n",
    "    for item in lst:\n",
    "        newList.append(item * 2)\n",
    "    return newList\n",
    "\n",
    "x = [4, 12, 3]\n",
    "x1 = doubleListFunc(x) # This solution doubles everything in the list even if we might only need 1st item\n",
    "print(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Write a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleListGen(lst):\n",
    "    for item in lst:\n",
    "        yield item * 2\n",
    "        \n",
    "x = [4, 12, 3]\n",
    "our_generator = doubleListGen(x)\n",
    "next(our_generator) # Gets excuted when the item is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(our_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(our_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(our_generator) # This will give an error. Generator got to the end of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a generator, if you need all the items, you will need a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4, 12, 3]\n",
    "x2 = []\n",
    "our_generator = doubleListGen(x)\n",
    "for x in our_generator:\n",
    "    x2.append(x)\n",
    "print(x2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4, 12, 3]\n",
    "our_generator = doubleListGen(x)\n",
    "x2 = [x for x in our_generator]\n",
    "print(x2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Sentences\n",
    "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc4.sents[0]\n",
    "for sent in doc4.sents:\n",
    "     print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4[6].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Barley Related Ag Data\n",
    "\n",
    "Instructions for extracting agricultural related data from descriptions of barley cultivars. Before extracting ag data, we need to add new named entities to the training model. These entities are: <br/>\n",
    "ALAS = varietal_alias <br/>\n",
    "CROP = crop <br/>\n",
    "CVAR = crop_variety <br/>\n",
    "JRNL = journal_reference <br/>\n",
    "PATH = pathogen <br/>\n",
    "PED  = pedigree <br/>\n",
    "PLAN = plant_anatomy <br/>\n",
    "PPTD = plant_predisposition_to_disease <br/>\n",
    "TRAT = trait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainNER import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to recognized ag data entities e.g., TRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the nature of out training dataset, we might get warnings that the \n",
    "# data is not well formatted. Ignore those errors for now. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NOTE: You need to change the value of output_dir to a directory in your system\n",
    "output_dir=\"temp\"\n",
    "# If you have high quality data, you can increase the number of iterations to 100. We \n",
    "# will use 10 for testing just to illustrate how the training works. \n",
    "n_iter = 10\n",
    "#trainModel(None,output_dir,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agdata_nlp = spacy.load(output_dir)\n",
    "# Add a custom function to the pipeline that fixes journal and PED entities. \n",
    "# agdata_nlp.add_pipe(compound_trait_entities, after='ner')\n",
    "# Confirm custom function was added to the pipeline. \n",
    "print(agdata_nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case\n",
    "test_text = '''Kold is a six-rowed winter feed barley and it was published in Crop Science 25:1123 (1985). \n",
    "It matures early and its awns are rough. It was selected from the \n",
    "cross Robust/6/Glenn/4/Nordic//Dickson/Trophy/3/Azure/5/Glenn/Karl.'''\n",
    "\n",
    "# Run spacy pipeline on test data\n",
    "doc = agdata_nlp(test_text)\n",
    "\n",
    "# Specify colors for the different entities to use with displacy\n",
    "colors = {'ALAS':'BlueViolet','CROP': 'Aqua','CVAR':'Chartreuse','PATH':'red','PED':'orange','PLAN':'pink','PPTD':'brown','TRAT':'yellow'}\n",
    "cust_options = {'ents': ['ALAS','CROP','CVAR','PATH','PED','PLAN','PPTD','TRAT'], 'colors':colors}\n",
    "\n",
    "# Visualize the entities using displacy. \n",
    "displacy.render(doc, style='ent', jupyter=True,options=cust_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on a PDF file\n",
    "### Open PDF file, extract one page, classify and display  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Open PDF file for reading\n",
    "pdfFile = open(\"Data/DavisLJ11/BarCvDescLJ11.pdf\", mode=\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "\n",
    "# Find the total number \n",
    "numPages = pdfReader.numPages\n",
    "\n",
    "# Randomly pick one if you want to. \n",
    "# pageNumber = random.randrange(0,numPages)\n",
    "\n",
    "# For now I will pick a specific page. \n",
    "pageNumber= 24\n",
    "\n",
    "\n",
    "# Get text\n",
    "OnePage = pdfReader.getPage(pageNumber)\n",
    "OnePageText = OnePage.extractText()\n",
    "\n",
    "# Close PDF file\n",
    "pdfFile.close()\n",
    "\n",
    "# Remove newlines. It appears multiple newlines together makes\n",
    "# Spacy think that is the end of a sentence. The PDF reader reads the text in\n",
    "# an odd fashion\n",
    "OnePageText = OnePageText.replace('\\n','')\n",
    "\n",
    "\n",
    "# Run spacy pipeline on test data\n",
    "doc = agdata_nlp(OnePageText)\n",
    "\n",
    "# Display entities if any were found. \n",
    "if doc.ents:\n",
    "        displacy.render(doc, style='ent', jupyter=True, options=cust_options)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
