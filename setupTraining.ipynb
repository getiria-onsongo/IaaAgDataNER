{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to speed up the process of having a human being name entities that they recognize in a paragraph, and having their positions in the paragraph identified and placed in a syntax usable by spaCy's NER training routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open PDF file, extract page two and display as sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  Syngenta Cereals.\n",
      "1 :  Dayn was the highest yielding hard white spring wheat over the past six years of the irrigated trials.\n",
      "2 :  Test weight is above average and heading date was at average.\n",
      "3 :  Protein was a little below average.\n",
      "4 :  Dayn was 2-3 inches taller than average but has good lodging resistance.\n",
      "5 :  End use quality is acceptable.\n",
      "6 :  Dayn is resistant to stripe rust and among the least susceptible hard white spring wheat for FHB.\n",
      "7 :  Duclair  \n",
      "8 :  a hard red spring developed and released by Montana AES, with solid stem characteristic that reduces impact from wheat stem sawfly.\n",
      "9 :  It is currently under testing for adaptability to southeast Idaho conditions for areas where wheat stem sawfly is a problem.\n",
      "10 :  Duclair is an awned semi-dwarf variety, similar to Choteau, but heading three day earlier and about 1-3 inches taller, depending on the year.\n",
      "11 :  Yield of Duclair in Soda Springs was very good in 2019, comparable to SY Selway (Table 38), with below average test weight and protein with an early heading date.\n",
      "12 :  Duclair is PVP protected.\n",
      "13 :  Glee (WA8074)  hard red spring wheat released in 2012 through Washington State University with desirable end use quality and resistance to stripe rust.\n",
      "14 :  Glee is included in the trial as a quality check.\n",
      "15 :  Yield of Glee is average in the dryland trials, lower than average under irrigation and similar to WB9411 (Table 33).\n",
      "16 :  Glee has good test weight, is taller than average (3-4 inches taller than WB9411) and is average for percent seed protein.\n",
      "17 :  Imperial  \n",
      "18 :  a durum grain variety with awns that turn black with maturity, Imperial yields were less than Alzada in the trials.\n",
      "19 :  Imperial had low test weight and high grain protein (Table 31, 33).\n",
      "20 :  Jefferson (IDO462)  \n",
      "21 :  hard red spring wheat released by Idaho AES and USDA-ARS in 1998.\n",
      "22 :  Jefferson is primarily intended as a  dryland variety due to it being taller than average (about four inches taller under irrigation) and susceptible to lodging.\n",
      "23 :  Irrigated and dryland yields have been at or above nursery averages\n",
      "24 :  (Table 31, 32).\n",
      "25 :  Jefferson has good quality when there is adequate soil nitrogen and sulfur, when it has a minimum of 13% grain protein.\n",
      "26 :  Jefferson is susceptible to the current races of stripe rust and very susceptible to FHB, but resistant to Hessian Fly.\n",
      "27 :  Klasic (NK77S1817)  \n",
      "28 :  a well-established hard white spring wheat with exceptional quality characteristics.\n",
      "29 :  Klasic was released  in 1982 by Northrup-King, and while yields in the extension trials are low, yields can be excellent with appropriate irrigation practices, especially early season.\n",
      "30 :  Klasic has good test weight, is 5-6 inches shorter than average, and is earlier in heading and maturity.\n",
      "31 :  Klasic is very susceptible to stripe rust, FHB and Cereal Cyst nematode.\n",
      "32 :  While in certain years, FHB symptom development may be low due to earlier heading, the DON toxins from FHB infection can be high, as in 2016 trials.\n",
      "33 :  Triazole fungicides applied at flowering are highly recommended as a standard practice in growing Klasic.\n",
      "34 :  Two additional entries of Klasic at higher seeding rates were added to determine effect of seeding rate on yield.\n",
      "35 :  Klasic was seeded at 1, 1.2 and 1.4 million seeds per acre in all irrigated spring trials.\n",
      "36 :  Averaged across all locations (Table 33), the seeding rates resulted in 97, 105, and 99 bu/A for 1, 1.2 and 1.4 million seeds per acre.\n",
      "37 :  Given an LSD (alpha = 0.05) of 18.3 bu/A, there is no significant increase in yield resulting from higher seeding rates.\n",
      "38 :  Net CL+ (WA8280 CL+)  \n",
      "39 :  a 2019 release from Washington State University, Net CL+\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "#import slate3k as slate\n",
    "# Use en_core_web_md across the board to be consistent. en_core_web_sm does\n",
    "# not contain the vectors we need for pre-training\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "filename = \"Data/UIdaho2019/small-grains-report_2019.pdf\"\n",
    "#filename = \"Data/CSU/Cowboy-reprint.pdf\"\n",
    "\n",
    "#Open PDF file for reading\n",
    "pdfFile = open(filename, mode=\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "\n",
    "# Select a page to work on\n",
    "pageNumber = 52\n",
    "\n",
    "# Get text\n",
    "OnePage = pdfReader.getPage(pageNumber-1) #0-based count\n",
    "OnePageText = OnePage.extractText()\n",
    "\n",
    "# Close PDF file\n",
    "pdfFile.close()\n",
    "\n",
    "# Replacement code to use slate which handles spacing within some PDFs better\n",
    "# with open(filename, 'rb') as f:\n",
    "#     OnePageText = slate.PDF(f).text()\n",
    "\n",
    "# Remove newlinesxmx82k-&a. It appears multiple newlines together makes\n",
    "# Spacy think that is the end of a sentence. The PDF reader reads the text in\n",
    "# an odd fashion\n",
    "OnePageText = OnePageText.replace('\\n','')\n",
    "\n",
    "# create a spaCy doc object from the page and break it into sentences\n",
    "doc = nlp(OnePageText)\n",
    "l=0\n",
    "for sent in doc.sents:\n",
    "     print(l, \": \", sent)\n",
    "     l = l+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Per-line named entity file and match entities to sentence positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "#fname = \"Data/DavisLJ11/barley_p\"+str(pageNumber)+\"_ner.txt\"\n",
    "fname = \"Data/UIdaho2019/small_grains_report_2019_p\"+str(pageNumber)+\"_ner.txt\" \n",
    "#fname = \"Data/CSU/Cowboy_p\"+str(pageNumber)+\"_ner.txt\"\n",
    "\n",
    "# Covert the nlp sentence generator into a list of sentences\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Open the file of manually matched pairs (sentence # <tab> word phrase <tab> named entity)\n",
    "# e.g.:\n",
    "#  0      AC Metcalfe     CVAR\n",
    "#  0      two-rowed       TRAT\n",
    "#  0      barley          CROP\n",
    "#  1      Agri-Food Candada   ORG\n",
    "#  1      1997    DATE\n",
    "file = open(fname)\n",
    "reader = csv.reader(file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "data = list()\n",
    "\n",
    "for row in reader:\n",
    "    try:\n",
    "        (sentIndex, phrase, label) = row\n",
    "        sent = sentences[int(sentIndex)].string.rstrip()\n",
    "        \n",
    "        # find all instances of the 'phrase' in the 'sent'.\n",
    "        iter = re.finditer(r\"\\b\"+phrase+r\"\\b\", sent)\n",
    "        indices = [m.start(0) for m in iter]\n",
    "        \n",
    "        # check to make sure the phrase the user said was there was indeed found\n",
    "        if len(indices) == 0:\n",
    "            raise ValueError\n",
    "                \n",
    "        # print out all instances\n",
    "        for i in indices:\n",
    "#            print(sentIndex, sent, phrase, \"(\"+str(i), i+len(phrase), \"'\"+label+\"')\")\n",
    "            data.append([sentIndex, sent, phrase, \"(\"+str(i)+\", \"+str(i+len(phrase))+\", '\"+label+\"')\"])\n",
    "        \n",
    "            \n",
    "    except:\n",
    "        print(\"Handle manually: \", row)\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame(data, columns = [\"Index\", \"Sentence\", \"Phrase\", \"MatchInfo\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to clean up overlapping intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "coordRegex = re.compile(r'(\\d+), (\\d+)')\n",
    "\n",
    "def sortByStart(coords):\n",
    "    \"\"\"For use in sort routines, return object with lowest (X,Y) values\"\"\"\n",
    "    # split out coordinates that come in as (5, 7, 'CVAR')\n",
    "    mo = coordRegex.search(coords)\n",
    "    return(int(mo.group(1)))\n",
    "\n",
    "def overlaps(coord1, coord2):\n",
    "    \"\"\"Check if coordinates of the form 5, 7, 'CVAR' and 32, 46, 'TRAT' overlap\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    "    \n",
    "    if ((coord1High >= coord2Low) and (coord1Low <= coord2Low) or\n",
    "        (coord2High >= coord1Low) and (coord2Low <= coord1Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def keepFirst(coord1, coord2):\n",
    "    \"\"\"Given overlapping coordinates, return the wider encompassing one.\"\"\"\n",
    "    mo1 = coordRegex.search(coord1)\n",
    "    mo2 = coordRegex.search(coord2)\n",
    "    coord1Low = int(mo1.group(1))\n",
    "    coord1High = int(mo1.group(2))\n",
    "    coord2Low = int(mo2.group(1))\n",
    "    coord2High = int(mo2.group(2))\n",
    " \n",
    "    if (int(coord1High) - int(coord1Low)) >= (int(coord2High) - int(coord2Low)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# print(\"Should be false:\", overlaps(\"(5, 7, 'CVAR')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(32, 46, 'TRAT')\"))\n",
    "# print(\"Should be true:\", overlaps(\"(26, 46, 'TRAT')\", \"(26, 46, 'TRAT')\"))\n",
    "# print(\"Keeper:\", keepFirst(\"34, 46, 'TRAT'\", \"34, 46, 'TRAT'\"))\n",
    "\n",
    "def cleanIntervals(inputString=\"\"):\n",
    "    \"\"\"order intervals like (5, 7, 'CVAR'), (32, 46, 'TRAT'), (26, 46, 'TRAT') and remove overlapping ones.\"\"\"\n",
    "    inputString = inputString.lstrip(\"(\").rstrip(\")\")\n",
    "    intervalList = inputString.split(\"), (\")\n",
    "    intervalList.sort(key = sortByStart)\n",
    "#    print(\"Sorted Interval List:\", intervalList)\n",
    "\n",
    "    # Pairwise compare every interval in the list to every other interval to check overlap\n",
    "    keeperList = [True]*len(intervalList) # Logic array to determine if each interval should be kept\n",
    "    i=0\n",
    "    for interval1 in intervalList:\n",
    "        for interval2 in intervalList:\n",
    "            if interval1 == interval2:\n",
    "                if intervalList.index(interval1) != i: # when both are the same we reject the higher one\n",
    "                    keeperList[i] = False\n",
    "            else:\n",
    "                if overlaps(interval1, interval2) and keepFirst(interval1, interval2) == False:\n",
    "                    keeperList[i] = False\n",
    "        i = i+1\n",
    "        \n",
    "#    print(\"keeperList:\", keeperList)\n",
    "   \n",
    "    # Build up the return interval list\n",
    "    returnStr = \"(\"\n",
    "    for interval, isKeeper in zip(intervalList, keeperList):\n",
    "        if isKeeper:\n",
    "            returnStr = returnStr + interval + \"), (\"\n",
    "    return (returnStr.rstrip(\"), (\") + \")\")\n",
    "        \n",
    "# cleanIntervals(\"(5, 7, 'CVAR'), (32, 46, 'TRAT'), (5, 9, 'CVAR'), (48, 55, 'ORG'), (26, 46, 'TRAT')\")\n",
    "# cleanIntervals(\"(0, 8, 'CVAR'), (0, 5, 'CVAR'), (21, 26, 'PLAN'), (32, 37, 'CVAR')\")\n",
    "#cleanIntervals(\"(0, 12, 'CVAR'), (39, 49, 'CVAR'), (39, 49, 'CVAR'), (71, 77, 'CVAR'), (71, 77, 'CVAR'), (92, 113, 'TRAT'), (140, 150, 'CVAR'), (140, 150, 'CVAR'), (181, 187, 'CVAR'), (181, 187, 'CVAR')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all matches for each sentence on a single line and output in spaCy training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use Pandas dataframes to aggregate all entity matches together for a single sentence\n",
    "agg_rules = {'Sentence': 'first', 'Phrase': 'first', 'MatchInfo': lambda x: ', '.join(x)}\n",
    "res = df.groupby('Index').agg(agg_rules)\n",
    "#print(res)\n",
    "\n",
    "# Now format it just like what is needed for the spaCy training module: \n",
    "# E.g.:\n",
    "# ('Eight-Twelve is a six-rowed winter feed barley', {'entities': [(0, 12, 'CVAR'), (18, 27, 'TRAT'), (28, 39, 'TRAT'),(40, 46, 'CROP')]}),\n",
    "records = res.to_dict('records')\n",
    "\n",
    "out_fname = \"Data/UIdaho2019/small_grains_report_2019_p\"+str(pageNumber)+\"_td.py\"\n",
    "fo = open(out_fname, 'w')\n",
    "fo.write(\"TRAIN_DATA = [\")\n",
    "maxr = len(records)\n",
    "for i in range(0,maxr):\n",
    "    fo.write(\"    ('\"+records[i]['Sentence']+\"', {'entities': [\"+cleanIntervals(records[i]['MatchInfo'])+\"]})\")\n",
    "    if (i == maxr-1):\n",
    "        fo.write(\"\\n\")\n",
    "    else:\n",
    "        fo.write(\",\\n\")\n",
    "\n",
    "fo.write(\"]\\n\")\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above content will be written to a file e.g., `Data/UIdaho2019/small_grains_report_2019_p50_td.py`. You can add any manual corrections (usually PED and JRNL entries) and then running the script `python3 py2json.py --doc 'Data/UIdaho2019/small-grains-report_2019.pdf' --url 'https://www.uidaho.edu/-/media/UIdaho-Responsive/Files/Extension/topic/cereals/scse/2019/small-grains-report_2019.pdf' --chunk 50 Data/UIdaho2019/small_grains_report_2019_p50_td.py Data/UIdaho2019/small_grains_report_2019_p50_td.json` to create the JSON file for Training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
