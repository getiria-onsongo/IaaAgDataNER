import getopt
import os
import re
import openai
import pickle
import tiktoken
from api import *
from prefix import *
from gemspdf2text import *


import requests
import sys
from htmldate import find_date
from time import mktime, strptime
from datetime import datetime

completion_prefix = """ """
currTime = datetime.now().isoformat()
token_buffer = 50
def count_tokens(input_prompt: str, input_model:str) -> int:
    """
        Because OpenAI limits the total number of tokens (prompt + completion), it is
        useful to know the total number of tokens your prompt will generate. Additionally,
        call cost (usage) is priced by token.

        This function takes as input a string and returns the number of tokens generated by
        tiktoken, an open-source tokenizer by OpenAI.
    """
    # Different models use different encodings. This method automatically retrieves
    # the correct encoding.
    encoding = tiktoken.encoding_for_model(input_model)

    # The .encode() method converts a text string into a list of token integers.
    tokens = encoding.encode(input_prompt)

    # Count the number of tokens
    num_tokens = len(tokens)
    return num_tokens


def make_completion_call(input_prompt, input_model, input_key, expected_output_tokens, response_path):
    """
        Takes as input a prompt and returns a completion from openAI.
    """
    openai.api_key = input_key
    response = openai.Completion.create(
      # model parameter specifies which model to use e.g., text-davinci-003
      model=input_model,
      # Prompt to generate completion for, encoded as a string
      prompt=input_prompt,
      max_tokens = expected_output_tokens,
      temperature=0,
      top_p=1.0,
      frequency_penalty=0.0,
      presence_penalty=0.0
    )

    with open(response_path, 'wb') as file:
        pickle.dump(response, file)
        print(f'OpenAI response successfully saved to "{response_path}"')
    return response


def extract_single_call(esc_text, esc_input_model, esc_input_key, esc_expected_out_tokens, esc_source, esc_dir, esc_name_base, file_path):
    # prompt_prefix : This is a global variable declared in prefix.py
    input_prompt = prompt_prefix + esc_text

    # Path to file containing response from OpenAI API
    # (update to use the os module to be platform independent: os.path.join(os.getcwd(), file_name)
    esc_response_path = esc_dir + "/" + esc_name_base + ".pkl"

    if os.path.isfile(esc_response_path):
        print(f'Looks like an API call was made for the same file name: {esc_response_path},')
        print(f'Delete or rename the file to make a new API request to be saved in the same file.')
        response = load_openai_response(esc_response_path)
    else:
        response = make_completion_call(input_prompt, esc_input_model, esc_input_key, esc_expected_out_tokens, esc_response_path)

    #print("response=\n", response["choices"][0]["text"])
    annotation = parse_completion_call_response(response, esc_source, file_path)
    return annotation


def extract_multiple_calls(emc_text, emc_input_model, emc_input_key, emc_expected_out_tokens, emc_source, emc_dir, emc_name_base, file_path):
    # prompt_prefix_smaller : This is a global variable declared in prefix.py

    base_tokens = count_tokens(prompt_prefix, emc_input_model) + emc_expected_out_tokens
    current_tokens = base_tokens
    cnt = 0
    local_text = ""
    annotation = []

    text_array = emc_text.replace("\n", " ").split(". ")

    for row in text_array:
        if (current_tokens + count_tokens(row, emc_input_model)) < context_length:
            local_text = local_text + row + ". "
            current_tokens = current_tokens + count_tokens(row, emc_input_model)
        else:
            # print("local_text=", local_text)
            input_prompt = prompt_prefix + local_text
            emc_response_path = emc_dir + "/" + str(cnt) + "_" + emc_name_base + ".pkl"
            if os.path.isfile(emc_response_path):
                print(f'Looks like an API call was made for the same file name: {emc_response_path},')
                print(f'Delete or rename the file to make a new API request to be saved in the same file.')
                response = load_openai_response(emc_response_path)
            else:
                response = make_completion_call(input_prompt, emc_input_model, emc_input_key, emc_expected_out_tokens,
                                                emc_response_path)

            annotation.extend(parse_completion_call_response(response, emc_source, file_path))
            current_tokens = base_tokens
            local_text = ""
            cnt = cnt + 1

    # print("local_text=", local_text)
    input_prompt = prompt_prefix + local_text
    emc_response_path = dir + "/" + str(cnt) + "_" + emc_name_base + ".pkl"
    if os.path.isfile(emc_response_path):
        print(f'Looks like an API call was made for the same file name: {emc_response_path},')
        print(f'Delete or rename the file to make a new API request to be saved in the same file.')
        response = load_openai_response(emc_response_path)
    else:
        response = make_completion_call(input_prompt, emc_input_model, emc_input_key, emc_expected_out_tokens,
                                        emc_response_path)

    annotation.extend(parse_completion_call_response(response, emc_source, file_path))
    return annotation


def extract_data(input_model, input_key, dir, source, out_file_name):

    # sample_output : This is a global variable declared in prefix.py
    # context_length : This is a global variable declared in prefix.py

    file_path = download_file(source, dir)
    extension = file_path.split(".")[-1].lower().strip()
    # name_base = os.path.basename(file_path).split(".")[0]
    name_base = os.path.basename(file_path)

    ed_text = ""
    doc_date = "not found"
    if extension == 'pdf':
        ed_text = textract.process(file_path, method='pdfminer').decode('utf-8').strip().lower()
        doc_date = get_pdf_date(file_path)
    elif extension == 'html':
        # For HTML, we need to extract date from the URL. Downloading the actual HTML does work some of the time.
        # This might be a function of how the HTML is generated but need to be investigated. At the moment, working
        # with the actual URL appears to get around the issue.
        doc_date = get_html_date(source)
        ed_text = textract.process(file_path, extension='html').decode('utf-8').strip().lower()
    else:
        sys.exit('File extension not recognized. Please use .pdf or .html extensions.')

    # print("doc_date=", doc_date)

    # Path to file containing extracted data
    data_file_path = dir + "/" + name_base + ".txt"

    # Ignore reference section if it exists
    clean_text = ""
    temp_text = ed_text.split('\nreferences\n')
    if len(temp_text) > 1:
        clean_text = temp_text[0]
    else:
        clean_text = ed_text

    expected_out_tokens = count_tokens(sample_output, input_model) + token_buffer

    input_prompt = prompt_prefix + clean_text
    input_prompt_tokens = count_tokens(input_prompt, input_model)
    expected_tokens = input_prompt_tokens + expected_out_tokens

    if expected_tokens < context_length:
        print("Go to plan A")
        data = extract_single_call(clean_text, input_model, input_key, expected_out_tokens, source, dir, name_base, file_path)

    else:
        print("Go to plan B")
        data = extract_multiple_calls(clean_text, input_model, input_key, expected_out_tokens, source, dir, name_base, file_path)

    nodup_data = remove_list_duplicates(data)
    nodup_data.sort()

    with open(out_file_name, 'a') as output_file:
        for val in nodup_data:
            print(val, file=output_file)

def parse_completion_call_response(data, input_source, file_path):
    """
    Takes as input text submitted to openAI and completion response from OpenAI. It parses the results
    returning a tab delimited file that can be easily loaded into a database. This function assumes the results
    are for extracting crop variety information from the text.

    Below is sample output that needs to be parsed:

    Variety Name: Tamalpais
    Variety Aliases: UCD YP03-9/3 # Entry 1134
    Crop Name: Barley
    Germplasm Characteristics: Hulless (naked) feed/food barley
    Crop Variety Class: Six-rowed spring hulless feed/food barley
    Journal References: California AES (2007).
    Pathogens: Leaf rust # powdery mildew # scald # net blotch # BYD # stripe rust
    Pedigrees: Ataco/Achira//Higo x UC 960
    Traits: Medium maturing # medium-short height # good straw strength # full, rough awns # good resistance to shattering # beige (non-blue, transparent aleurone which is classified as white) # high in Beta-glucan (above 6%).
    Plant Predisposition to Diseases: Resistant to leaf rust and powdery mildew # moderately resistant to scald, net blotch, and BYD # moderately susceptible to stripe rust.

    Below are the first three lines of the expected output:

    tamalpais   variety aliases    ucd yp03-9/3
    tamalpais   variety aliases    entry 1134
    tamalpais   crop name          barley
    """
    # variety_name_key : This is a global variable declared in prefix.py
    response_text = data["choices"][0]["text"].splitlines()
    dump_data = 0
    local_data_dictionary = {}
    extracted_data = []

    for line in response_text:
        cols = line.lower().strip().split(":")
        # If this was a valid response, it will be of the form "Variety Name : Avery"
        if len(cols) > 1:
            key = cols[0].lower().strip()
            value = cols[1].lower().strip()
            if key == variety_name_key:
                if dump_data == 0:
                    dump_data = 1
                else:
                    current_data = append_variety_name(variety_name_key, local_data_dictionary, input_source, file_path)
                    if len(current_data) > 0:
                        extracted_data.append(current_data)
                    local_data_dictionary = {}

            # We asked the model to use # as the delimiter if more than one value was returned
            values = value.split("#")

            results = []
            if len(values) > 1:
                for val in values:
                    results.append(val.strip())
            else:
                results.append(value.strip())

            local_data_dictionary[key.strip()] = results

    # If there was only one variety name found, the if block that extracts data will not have been reached. We
    # need to extract data
    current_data = append_variety_name(variety_name_key, local_data_dictionary, input_source, file_path)
    if len(current_data) > 0:
        extracted_data.extend(current_data)
    return extracted_data


def append_variety_name(variety_name_key, input_data_dictionary, input_source, file_path):
    """ Add documentation """
    # missing_symbol: This is a global variable declared in prefix.py
    output = []
    name_base = os.path.basename(file_path)
    if variety_name_key in input_data_dictionary:
        variety_name = input_data_dictionary[variety_name_key][0]
        for (key, values) in input_data_dictionary.items():
            for value in values:
                if value != missing_symbol:
                    output.append(currTime + "\t" + name_base + "\t" + variety_name + "\t" + key + "\t" + value.strip())
        output.append(currTime + "\t" + name_base + "\t" + variety_name + "\tsource\t" + input_source)
        output.append(currTime + "\t" + name_base + "\t" + variety_name + "\tlocal_path\t" + file_path)
        output.sort()
    return output


def download_file(url: str, dir:str) -> str:
    """ Download file from given URL to local directory.

    :param url: The url of the file to be downloaded
    :return: local path to file if successfully downloaded, otherwise empty string.
    """

    # isolate filename from URL
    url_parts = url.split("/")

    if len(url_parts[-1]) > 0:
        file_name = url_parts[-1]
    elif len(url_parts[-2]) > 0:
        file_name = url_parts[-2]
    else:
        file_name = "NoName"

    ext = check_file_type(url)
    # update to use the os module to be platform independent: os.path.join(os.getcwd(), file_name
    if ext == ".html":
        file_path = dir + "/" + file_name + ".html"
    elif ext == ".pdf":
        if ".pdf" in file_name:
            file_path = dir + "/" + file_name
        else:
            file_path = dir + "/" + file_name + ".pdf"
    else:
        sys.exit("The file in your source is currently not supported")

    # Request URL and get response object
    response = requests.get(url, stream=True)

    if response.status_code == 200:
        with open(file_path, 'wb') as file_object:
            file_object.write(response.content)
            print(f'{file_name} was successfully saved!')
    else:
        print(f'Uh oh! Could not download {file_name},')
        print(f'HTTP response status code: {response.status_code}')

    return file_path


def load_openai_response(file_path: str):
    """ Load OpenAI completion API response that was saved using pickle.

    :param file_path: Path to saved data
    :return: text that was returned by the API.
    """

    # open a file, where you stored the pickled data
    file = open(file_path, 'rb')
    # dump information to that file
    data = pickle.load(file)
    # close the file
    file.close()

    return data


def get_pdf_date(file_path):
    """ Add documentation. """
    fp = open(file_path, 'rb')
    parser = PDFParser(fp)
    doc = PDFDocument(parser)
    creation_date = doc.info[0]['CreationDate'][2:-7].decode()

    ts = strptime(creation_date, "%Y%m%d%H%M%S")
    dt = datetime.fromtimestamp(mktime(ts))
    return str(dt.year) + "-" + str(dt.month) + "-" + str(dt.day)

def get_html_date(url):
    """ Add documentation. """
    html_date = find_date(url)
    return html_date


def remove_list_duplicates(input_list):
    """ Add documentation. """
    dedup = []
    for item in input_list:
        if item not in dedup:
            dedup.append(item)
    return dedup


def print_parsed_output(data):
    variety_name = data[0]
    variety_annotation = data[1]

    for val in variety_annotation:
        if (val[0] != 'doc_name' and val[0] != 'source_text'):
            print(variety_name, "\t", val[0], "\t", val[1])


def check_file_type(source):
    r = requests.get(source)
    content_type = r.headers.get('content-type')
    if 'application/pdf' in content_type:
        ext = '.pdf'
    elif 'text/html' in content_type:
        ext = '.html'
    else:
        ext = ''
        print('Unknown type: {}'.format(content_type))

    return ext


if __name__ == "__main__":
    api_model = "text-davinci-003"

    # OPENAI_API_ORG is an environment variable set by bash_profile.sh
    api_org = os.getenv("OPENAI_API_ORG")

    # OPENAI_API_KEY is an environment variable set by bash_profile.sh
    api_key = os.getenv("OPENAI_API_KEY")


    dir = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test"

    # file_source = "https://agsci.colostate.edu/wheat/wp-content/uploads/sites/85/2016/02/Bond-CL-Reprint.pdf"
    # file_source = "https://www.canr.msu.edu/potatobg/Files/Potato-Varieties/BoulderProfile.pdf"
    # file_source = "https://edis.ifas.ufl.edu/publication/HS1296"
    # file_source = "https://agsci.colostate.edu/wheat/wp-content/uploads/sites/85/2019/03/Incline-AX.pdf"
    # file_source = "https://seedpotato.russell.wisc.edu/2019/06/25/adirondack-blue-fact-sheet/"
    # file_source = "https://seedpotato.russell.wisc.edu/2019/06/27/clearwater-russet-fact-sheet/"
    # file_source = "https://seedpotato.russell.wisc.edu/2018/05/14/yukon-gold-potato-fact-sheet-3/"
    # file_source = "https://potatoassociation.org/varieties/white-varieties/yukon-gold-solanum-tuberosum/"
    file_source = "https://cropwatch.unl.edu/potato/yukongold_characteristics"

    # The Examples below are a bit more complex. More time needs to be devoted into figuring out
    # instances where a variety name wasn't in the phrase. NOTE: This could problem would be
    # easier to solve if we do not allow summary documents with more than one variety. If we add this
    # restriction, we just need a variety name to be identified somewhere in the document.

    # ToDO: We need to pre-process the PDF files and remove Acknowledgement and References so they don't get
    # included.

    # file_source = "http://smallgrains.ucdavis.edu"
    # file_source = "https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=2062&context=extensionhist"
    # file_source = "https://www.k-state.edu/wgrc/publications/2003/8175.pdf"
    # file_source = "https://core.ac.uk/download/pdf/158353049.pdf"
    # file_source = "https://seedpotato.russell.wisc.edu/2019/06/25/adirondack-blue-fact-sheet/"

    out_file_name = ""
    if len(sys.argv) > 1:
        out_file_name = sys.argv[1]
    out_dir = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/postgres"
    output_file = out_dir + "/" + out_file_name
    # -------- extract_data(api_model, api_key, dir, file_source, output_file)

    # --------- print(get_crop_varieties("potato"))

    # -------- getInfo(cropName, varietyName)
    # give info on what year the variety was introduced, what institution produced it, where geographically was it produced
    # -------- print(get_crop_info('potato', 'yukon gold'))

    # -------  getTraits(cropName, varietyName)
    # -------- give me all the traits of adirondack.
    #

    # ------ print(get_traits('potato', 'yukon gold'))

    # --------print(get_source('wheat', 'incline ax', 'excellent resistance to stripe rust'))
    #------- print(get_file_name(4))

    cropName = 'potato'
    trait_inclusion_list = ['resistant to verticillium wilt', 'white creamy flesh']
    trait_exclusion_list = ['susceptible to common scab']
    print(match_traits(cropName , trait_inclusion_list, trait_exclusion_list))
    # --------- matchTraits(cropName, traitInclusionList[], TraitExclusionList[])

    """
    source = "https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=2062&context=extensionhist"
    file_path = download_file(source, dir)
    print(file_path)

    source = "https://www.mncia.org/umn-releases-mn-rothsay-wheat/"
    file_path = download_file(source, dir)
    print(file_path)

    source = "https://agsci.colostate.edu/wheat/wp-content/uploads/sites/85/2020/01/canvas.pdf"
    file_path = download_file(source, dir)
    print(file_path)

    source = "https://seedpotato.russell.wisc.edu/2018/05/14/yukon-gold-potato-fact-sheet-3/"
    file_path = download_file(source, dir)
    print(file_path)
    """

    """
    file_path ="/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/yukon-gold-potato-fact-sheet-3.html"
    ed_text = textract.process(file_path, extension='html').decode('utf-8').strip().lower()
    print(ed_text)
    """

    """
    file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/Blackberry%20profile.pdf"
    ed_text = textract.process(file_path, method='pdfminer').decode('utf-8').strip().lower()
    print(ed_text)
    """

    """
    file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/yukon-gold-potato-fact-sheet-3.html"
    name_base = os.path.basename(file_path)
    print(name_base)

    file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/viewcontent.cgi?article=2062&context=extensionhist.pdf"
    name_base = os.path.basename(file_path)
    print(name_base)
    """

    # Try to remove acknowledgement and references

    # print("ed_text = \n", ed_text)

    # dir = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test"
    # fileName = "testFour.html"
    # outputFileName = "testFour_output.pkl"
    # text = textract.process(dir+"/"+fileName, extension='html').decode('utf-8').strip()
    # print("-------------------------------------------------- \n")
    # print(text)
    # print("-------------------------------------------------- \n")
    # file_source = dir + "/" + "AdirondackBlue.html"
    # file_source = "https://www.webstaurantstore.com/article/572/types-of-potatoes.html#russet"
    # file_source = "https://seedpotato.russell.wisc.edu/2019/06/25/bannock-russet/"
     # date = find_date(file_source)
    # print("Date printed=", date)



    # file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/Bond-CL-Reprint.pdf"
    # file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/testFive.pdf"
    # file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/NF96-307.pdf"
    # file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/8175.pdf"
    # file_path = "/Users/gonsongo/Desktop/research/gems/IaaAgDataNER/Data/test/158353049.pdf"
    # ed_text = textract.process(file_path, method='pdfminer').decode('utf-8').strip().lower()













